{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Contextual_bandit():\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        #List out bandits. Currently arms 4, 2, and 1 (respectively) are the most optimal.\n",
    "        self.bandits = np.array([[0.2,0,0,-5], [0.1,-5,1,0.25], [-5,5,5,5]])\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "    \n",
    "    def getBandit(self):\n",
    "        self.state = np.random.randint(0, self.num_bandits)\n",
    "        return self.state\n",
    "    \n",
    "    def pullArm(self, action):\n",
    "        bandit = self.bandits[self.state, action]\n",
    "        result = np.random.randn(1)    #正态分布\n",
    "        if result > bandit:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, learning_rate=0.001, state_size=3, action_size=4):\n",
    "        self.state_in = tf.placeholder(dtype=tf.int32)\n",
    "        self.weights = tf.Variable(tf.ones([state_size, action_size]))\n",
    "        self.output = tf.sigmoid(self.weights[self.state_in]) \n",
    "        self.chosen_action = tf.argmax(self.output, 0)\n",
    "        \n",
    "        self.reward_ = tf.placeholder(dtype=tf.float32)\n",
    "        self.action_ = tf.placeholder(dtype=tf.int32)\n",
    "        self.responsible_weight = self.output[self.action_]\n",
    "        self.loss = -(tf.log(self.responsible_weight) * self.reward_)\n",
    "        self.opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Running reward : \n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "Episode 501, Running reward : \n",
      "[[  -3.    0.    0.  130.]\n",
      " [   1.  141.   -6.   -1.]\n",
      " [ 161.   -5.   -5.   -4.]]\n",
      "Episode 1001, Running reward : \n",
      "[[  -5.    2.   -5.  275.]\n",
      " [   1.  319.   -9.    0.]\n",
      " [ 305.   -7.   -9.   -6.]]\n",
      "Episode 1501, Running reward : \n",
      "[[  -7.    2.   -9.  432.]\n",
      " [  -2.  461.  -12.    1.]\n",
      " [ 470.   -9.  -12.  -12.]]\n",
      "Episode 2001, Running reward : \n",
      "[[  -9.    4.   -8.  571.]\n",
      " [  -3.  626.  -13.    2.]\n",
      " [ 622.  -16.  -17.  -18.]]\n",
      "Episode 2501, Running reward : \n",
      "[[  -9.    3.   -8.  704.]\n",
      " [  -2.  785.  -16.   -1.]\n",
      " [ 794.  -22.  -21.  -24.]]\n",
      "Episode 3001, Running reward : \n",
      "[[ -10.    2.   -9.  859.]\n",
      " [  -3.  947.  -15.   -5.]\n",
      " [ 951.  -24.  -24.  -28.]]\n",
      "Episode 3501, Running reward : \n",
      "[[  -11.     2.    -9.  1008.]\n",
      " [   -4.  1106.   -18.    -7.]\n",
      " [ 1095.   -36.   -32.   -33.]]\n",
      "Episode 4001, Running reward : \n",
      "[[  -12.     3.    -3.  1142.]\n",
      " [   -5.  1276.   -21.   -12.]\n",
      " [ 1242.   -41.   -36.   -38.]]\n",
      "Episode 4501, Running reward : \n",
      "[[  -12.     3.    -2.  1299.]\n",
      " [   -4.  1428.   -24.   -11.]\n",
      " [ 1395.   -45.   -42.   -42.]]\n",
      "Episode 5001, Running reward : \n",
      "[[ -1.20000000e+01   4.00000000e+00  -1.00000000e+00   1.44500000e+03]\n",
      " [ -6.00000000e+00   1.58700000e+03  -2.60000000e+01  -1.10000000e+01]\n",
      " [  1.55600000e+03  -5.00000000e+01  -4.50000000e+01  -4.60000000e+01]]\n",
      "Episode 5501, Running reward : \n",
      "[[  -10.     3.    -2.  1600.]\n",
      " [   -6.  1731.   -28.   -13.]\n",
      " [ 1709.   -57.   -52.   -54.]]\n",
      "Episode 6001, Running reward : \n",
      "[[   -9.     2.    -4.  1756.]\n",
      " [   -4.  1870.   -29.   -10.]\n",
      " [ 1878.   -62.   -56.   -57.]]\n",
      "Episode 6501, Running reward : \n",
      "[[   -8.     4.    -5.  1897.]\n",
      " [   -5.  2028.   -30.   -10.]\n",
      " [ 2040.   -64.   -61.   -61.]]\n",
      "Episode 7001, Running reward : \n",
      "[[ -9.00000000e+00   6.00000000e+00  -1.00000000e+00   2.05600000e+03]\n",
      " [ -7.00000000e+00   2.18000000e+03  -3.50000000e+01  -1.10000000e+01]\n",
      " [  2.19100000e+03  -6.50000000e+01  -7.00000000e+01  -6.40000000e+01]]\n",
      "Episode 7501, Running reward : \n",
      "[[  -10.     9.     0.  2202.]\n",
      " [   -4.  2338.   -41.   -11.]\n",
      " [ 2351.   -71.   -71.   -67.]]\n",
      "Episode 8001, Running reward : \n",
      "[[  -13.    15.     0.  2365.]\n",
      " [   -4.  2484.   -43.   -14.]\n",
      " [ 2500.   -77.   -75.   -73.]]\n",
      "Episode 8501, Running reward : \n",
      "[[ -1.30000000e+01   1.70000000e+01   2.00000000e+00   2.51300000e+03]\n",
      " [ -2.00000000e+00   2.63300000e+03  -4.50000000e+01  -1.30000000e+01]\n",
      " [  2.66100000e+03  -8.20000000e+01  -8.00000000e+01  -8.00000000e+01]]\n",
      "Episode 9001, Running reward : \n",
      "[[ -1.80000000e+01   1.70000000e+01   1.00000000e+00   2.65300000e+03]\n",
      " [ -1.00000000e+00   2.78100000e+03  -4.90000000e+01  -1.50000000e+01]\n",
      " [  2.83900000e+03  -8.60000000e+01  -8.40000000e+01  -8.10000000e+01]]\n",
      "Episode 9501, Running reward : \n",
      "[[ -1.90000000e+01   1.60000000e+01   1.00000000e+00   2.81600000e+03]\n",
      " [ -2.00000000e+00   2.93100000e+03  -5.10000000e+01  -1.70000000e+01]\n",
      " [  2.98600000e+03  -8.90000000e+01  -8.90000000e+01  -9.00000000e+01]]\n",
      "[[ 0.9954229   1.00376248  1.00054049  1.62419498]\n",
      " [ 0.99946356  1.64200723  0.9862169   0.99515229]\n",
      " [ 1.64880037  0.97475994  0.97558141  0.97475994]]\n",
      "The agent thinks bandit [4 2 1] is the most promising ......\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "cb = Contextual_bandit()\n",
    "agent =Agent()\n",
    "total_episodes = 10000\n",
    "total_reward = np.zeros([cb.num_bandits, cb.num_actions])\n",
    "e = 0.1\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(total_episodes):\n",
    "        state = cb.getBandit()\n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(cb.num_actions)\n",
    "        else:\n",
    "            action = sess.run(agent.chosen_action, feed_dict={agent.state_in:state})\n",
    "        reward = cb.pullArm(action)\n",
    "        \n",
    "        resp, ww, _ = sess.run([agent.responsible_weight, agent.weights, agent.opt], \n",
    "                               feed_dict={agent.action_:action, agent.reward_:reward, agent.state_in:state}) \n",
    "        \n",
    "        total_reward[state,action] += reward\n",
    "        if i% 500==0:\n",
    "            print(\"Episode {}, Running reward : \\n{}\".format(i+1, total_reward))\n",
    "            \n",
    "print(ww)\n",
    "print(\"The agent thinks bandit {} is the most promising ......\".format(np.argmax(ww,axis=1)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
